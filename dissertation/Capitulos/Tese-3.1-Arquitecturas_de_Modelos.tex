\subsubsection{Modelos \textit{machine learning} para previsão\label{se:arquiteturas_modelos}}

Grande parte da literatura sobre previsões em modelos de \textit{machine learning} apresenta as mesmas arquiteturas, sendo depois aprimoradas consoante os dados e o problema.\par
No presente trabalho, apresentar-se-ão as arquitecturas mais usadas em previsões, como também algumas usadas noutros ramos, com a finalidade de tentar prever a compatibilidade neste problema.\par
Neste trabalho vamos usar arquiteturas de \gls{FCNN}, \gls{CNN}, \gls{LSTM} e \textit{Transformer}.\par




\paragraph{FCNN\label{se:fcnn_sec}}
\text{ }  \par

A arquitetura mais simples \gls{FCNN}, Redes Neuronais Totalmente Conectadas, é constituída por camadas em que cada neurónio está ligado a todos os neurónios da camada seguinte. Isto significa que cada caraterística de entrada tem um peso associado, e esses pesos são aprendidos durante o treino. A saída de cada neurónio é calculada através da aplicação de uma função de ativação à soma ponderada das suas entradas.\par
Cada neurónio gera uma operação, inicialmente aleatória, para tentar reproduzir uma função que traduza a entrada na saída ideal.\par
Esta arquitectura tem como base o Perceptão inicialmente proposto em \cite{Rosenblatt1958}. Este apresentava um Perceptão que fazia uma decisão binária baseado nas somas pesadas de todas as entradas.\par
A ideia é a base utilizada actualmente, mas apresentava algumas limitações, e muita computação, o proposto por \cite{Minsky1969}, eleva a ideia com a introdução da função de activação e o bias. Actualmente os neurónios mais usados têm por base o proposto em \cite{Haykin1999}:


\begin{figure}[H]
	\centering
	\resizebox{0.7\linewidth}{!}{\input{graphs/neuronio.tex}}
	\caption{Ilustração de um neurónio. Adaptado de \cite{Haykin1999}}
	\label{fig:neuronio}
\end{figure}



\paragraph{CNN\label{se:cnn_sec}}
\text{ }  \par

As Redes Neuronais Convolucionais (\gls{CNN}) diferem das \gls{FCNN} no sentido em que os filtros (neurónios) não são criados aleatoriamente, mas cada filtro trata de uma parte da camada de entrada. Nas convoluções é criada uma janela móvel que percorre a camada, criando um saída desse conjunto de pontos. Esta janela move-se sempre subsequentemente.\par
Esta operação é normalmente feita na dimensão (ou dimensões) em que queremos perceber padrões.\
Nos nossos dados a convolução será na dimensão temporal.\par
Se tivermos uma matriz com nove passos temporais (N,9,1), se o tamanho da janela de convolução for 3, teremos uma saída de tamanho 6 (N, 6, 1).\par
\begin{figure}[H]
	\centering
	\resizebox{0.7\linewidth}{!}{\input{graphs/conv1D.tex}}
	\caption{Ilustração da operação de Convolução}
	\label{fig:conv_layer1D}
\end{figure}

Anteriormente ignoramos o número de filtros. Mas as convoluções criam o número pedido de filtros para cada janela temporal. Aqui cada filtro vai funcionar como na camada \gls{FCNN}, onde cada um começa com uma operação pseudo aleatória. Esta operação normalmente é feita na dimensão dos atributos.\par
Ou seja, a quantidade de filtros que esta camada irá produzir por convolução.\par
Se tivermos a mesma entrada que anteriormente mas com 4 atributos (N, 9, 4), e se definir o número de filtros para 2 teremos uma saída (N, 6, 2).\par
Ou seja, dois filtros por cada janela temporal.\par


\begin{figure}[H]
	\centering
	\resizebox{0.7\linewidth}{!}{\input{graphs/conv_layer.tex}}
	\caption{Ilustração da camada de Convolução}
	\label{fig:conv_layer}
\end{figure}

As convoluções podem realizar as operações em mais dimensões, é comum usar 2D para imagens, e 3D para vídeos. Neste trabalho apenas trabalhamos com convoluções 1D.\par

\subparagraph{UNET\label{se:unet_sec}}
\text{ }  \par
Num desenho especial de \gls{CNN}, normalmente usando em modelação de imagens, e primeiro proposto em \cite{Shelhamer2014}, a arquitectura UNET passa por criar uma rede de expansão dos filtros, usando convoluções, e de seguida uma rede de contracção dos mesmo, até aos tamanhos pretendidos.\par
Nas suas ligações, a arquitectura UNET junta informação de filtros passados (não de nível temporal mas de rede neuronal) para realçar informação já trabalhada, e assim identificar padrões de vários contextos diferentes.\par
É assim designada pois é uma rede (NET) que forma um U na sua expansão, contracção e ligações entre estes.\par
Em cada camada de \textit{encoding} vão sendo usadas convoluções para criar novos filtros e diminuir a dimensionalidade, enquanto que na fase de \textit{decoding} são usadas convoluções para aumentar a dimensionalidade e diminuir o número de filtros, adicionando a camada \textit{decoder} de tamanho análogo.\par

\begin{figure}[H]
	\centering
	\resizebox{\linewidth}{!}{\input{graphs/unet.tex}}
	\caption{Ilustração uma rede UNET.}
	\label{fig:unet_graph}
\end{figure}


\paragraph{RNN\label{se:rnn_sec}}
\text{ }  \par

As Redes Neuronais Recorrentes (RNN) são projetadas para processar sequências de dados, onde a ordem dos elementos é fundamental. Estas funcionam transmitindo informações de um neurónio para outro numa cadeia, o que permite que cada neurónio seja influenciado pelo estado anterior da rede.\par
Esta operação é feita através de \textit{loops} internos que permitem à rede "memorizar" informações das etapas anteriores. No entanto, as RNNs enfrentam dificuldades ao tentar lembrar informações de longo prazo, devido ao problema conhecido como desvanecimento do gradiente, onde os gradientes se tornam muito pequenos e impedem a actualização eficaz dos pesos da rede.\par

\subparagraph{LSTM\label{se:lstms_sec}}
\text{ }  \par

As redes \gls{LSTM} são um tipo especial de \gls{RNN} projetado para superar os problemas de memória de longo prazo encontrados nas \gls{RNN}s. Tal é conseguido através de uma estrutura de célula que mantém informações ao longo do tempo, permitindo que a rede memorize detalhes importantes mesmo após muitos passos no tempo.\par
As \gls{LSTM}s usam mecanismos de portão para controlar o fluxo de informações, permitindo a desconsideração de informações irrelevantes e a manutenção das informações relevantes. Esta característica torna-as particularmente eficazes em tarefas que exigem o entendimento de dependências de longo prazo em dados sequenciais.\par


O uso de \gls{LSTM} para previsões é uma área comum, mas aqui é seguido através das ideias partilhadas em \cite{Hewamalage2021}, e reforçado pelo uso em previsões energéticas demonstradas em \cite{Costa2022}.\par


\paragraph{\textit{Transformer}\label{se:transformer_sec}}
\text{ }  \par

Os \textit{Transformers} são um tipo de arquitetura de modelo que utiliza mecanismos de atenção para pesar a importância de diferentes partes de um dado de entrada, primeiro apresentado em \cite{Vaswani2017}.\par
Ao invés de processar os dados sequencialmente, como sucede nas RNNs, os \textit{Transformers} processam todos os elementos do dado de entrada simultaneamente,através de um mecanismo de atenção que calcula uma pontuação de atenção para cada par de elementos no dado de entrada, indicando quão relevante um elemento é para o outro. Estas pontuações de atenção são então usadas para ponderar a contribuição de cada elemento no resultado final.\par
Esta característica permite aos \textit{Transformers} capturar dependências de longo alcance nos dados de forma eficiente, tornando-os extremamente eficazes para tarefas de processamento de linguagem natural, como tradução automática e sumarização de texto.\par
% TODO: ref para cahtgpt e dall-e e assim
Este tipo de desenho é a base para os modelos generativos mais conhecidos como o \textit{chatGPT} para linguagem ou o \textit{Dall-E} para imagens.\par



