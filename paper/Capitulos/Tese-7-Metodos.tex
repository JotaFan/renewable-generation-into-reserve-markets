\chapter{Métodos}

% Neste capitulo percorremos as experiências realizadas. Estas foram feitas atraves do usos do programas criados para o efeito, disponiveis no repositorio GitHub do \href{https://github.com/JotaFan/renewable-generation-into-reserve-markets}{projecto}.\\

% \section{Benchmark}

% Como modelo bencharmark iremos usar a alocação feita. Pois são estes valores que procuramos melhorar no caso prático.\\



% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/benchmark.png}
%     \caption{Serie Temporal do benchmark}
%     \label{fig:benchmark}
% \end{figure}
  

% \begin{table}[H]
%     \caption{Dados Benchmark}
%     \resizebox{\linewidth}{!}{\csvautotabular{../data/benchmark_scores.csv}\label{tb:benchmark}}       
% \end{table}


% Para validação dos mesmo, vamos usar o ano 2021, devido aquele salto nos valores de alocação em 2022.\\

% Para esses temos os seguintes dados:

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/benchmark_validation.png}
%     \caption{Serie Temporal do benchmark 2021}
%     \label{fig:benchmark_validation}
% \end{figure}
  

% Os metodos em estudo vão ser comparados a esta medida. Sendo que o principal é baixar tanto a alocação perdida, como a alocação a mais. Que se traduzem no erro absoluto.\\

% \begin{table}[H]
%     \caption{Dados Benchmark de validação}
%     \resizebox{\linewidth}{!}{\csvautotabular{../data/benchmark_validation_scores.csv}  \label{tb:benchmark_val}}      
% \end{table}

% \section{Modelos estatiscos\label{se:model_stats}}

% Antes de entrar para o densenvolvimento de modelos vamos usar metódos e modelos abertos para usar comparativamente.\\

% Os modelos estatiscos recurrentes em previsões são AR, MA, ARMA, ARIMA, SARIMA para previsões so com um atributo, e para multiplos atributos VAR.\\
% O modelo AR e o VAR não obtiveram resultados aplicaveis, logo foram desconsiderado.\\

% \subsection{Univariate Analysis}

% Estas análises apenas aplicam uma formula à variavel em questão.

% \subsubsection{AR}
% \begin{equation} \label{eq:AR} 
%     y_t = \beta_0 + \beta_1 y_{t-1} + \dots + \beta_p y_{t-p} + \epsilon_t 
% \end{equation}
% onde:
% \begin{itemize}
%   \item $y_t$: O valor da serie no tempo $t$.
%   \item $p$: O número de atrasos.
%   \item $\epsilon_t$: O barulho no tempo $t$.
%   \item $\beta$: O coeficiente dos valores em atrasdo.
% \end{itemize}

% \subsubsection{MA}
% MA - Moving Average

% A MA 


% \begin{equation} \label{eq:MA} 
%     y_t = c + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}
% \end{equation}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/MA_model.png}
%     \caption{Previsões 2021 com modelo MA}
%     \label{fig:MA_model}
% \end{figure}
  
% \subsection{ARMA}

% AR eé blabla

% \begin{equation} \label{eq:ARMA}  y_t = \beta_0 + \beta_1 y_{t-1} + \dots + \beta_p y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}  \end{equation}
% \textbf{ARMA (Autoregressive Moving Average) Model:}
% \begin{itemize}
%   \item{$y_t$: The value of the time series at time $t$.}
%   \item $p$: The number of time lags to regress on (AR part).
%   \item $q$: The number of time lags of the error term to regress on (MA part).
%   \item $\epsilon_t$: The error term at time $t$.
%   \item $\beta$: The coefficients of the lagged values (AR part).
%   \item $\theta$: The coefficients of the lagged error terms (MA part).
% \end{itemize}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/ARMA_model.png}
%     \caption{Previsões 2021 com modelo ARMA}
%     \label{fig:ARMA_model}
% \end{figure}

% \subsection{ARIMA}

% AR eé blabla

% \begin{equation} \label{eq:ARIMA}y_t^d = \beta_0 + \beta_1 y_{t-1}^d + \dots + \beta_p y_{t-p}^d + \epsilon_t^d + \theta_1 \epsilon_{t-1}^d + \dots + \theta_q \epsilon_{t-q}^d \end{equation}
% \textbf{ARIMA (Autoregressive Integrated Moving Average) Model:}
% \begin{itemize}
%   \item $y_t^{[d]}$: The differenced value of the time series at time $t$.
%   \item $p$: The number of time lags to regress on (AR part).
%   \item $d$: The order of differencing.
%   \item $q$: The number of time lags of the error term to regress on (MA part).
%   \item $\epsilon_t^{[d]}$: The differenced error term at time $t$.
%   \item $\beta$: The coefficients of the lagged values (AR part).
%   \item $\theta$: The coefficients of the lagged error terms (MA part).
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/ARIMA_model.png}
%     \caption{Previsões 2021 com modelo ARIMA}
%     \label{fig:ARIMA_model}
% \end{figure}

% \subsection{SARIMA}

% AR eé blabla



% AR eé blabla

% \begin{equation} \label{eq:SARIMA} y_t^{[m]d} = \beta_0 + \beta_1 y_{t-m}^{[m]d} + \dots + \beta_p y_{t-pm}^{[m]d} + \epsilon_t^{[m]d} + \theta_1 \epsilon_{t-m}^{[m]d} + \dots + \theta_q \epsilon_{t-qm}^{[m]d} \end{equation}

% \textbf{SARIMA (Seasonal Autoregressive Integrated Moving Average) Model:}
% \begin{itemize}
%   \item $y_t^{[m]d}$: The differenced value of the time series at time $t$.
%   \item $p$: The number of time lags to regress on (AR part).
%   \item $d$: The order of differencing.
%   \item $q$: The number of time lags of the error term to regress on (MA part).
%   \item $m$: The number of time lags comprising one full period of seasonality.
%   \item $\epsilon_t^{[m]d}$: The differenced error term at time $t$.
%   \item $\beta$: The coefficients of the lagged values (AR part).
%   \item $\theta$: The coefficients of the lagged error terms (MA part).
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/SARIMA_model.png}
%     \caption{Previsões 2021 com modelo SARIMA}
%     \label{fig:SARIMA_model}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/VAR model_model.png}
%     \caption{Previsões 2021 com modelo VAR}
%     \label{fig:VAR_model}
% \end{figure}









% \subsection{Resultados\label{se:statitics_scores}}

% \begin{table}[H]
%     \caption{Resultados modelos Estatísticos}
%     \resizebox{\linewidth}{!}{\input{../statistical_models/stats_scores.tex}\label{tb:statitics_scores}}      
% \end{table}

% Apenas pelos métodos estatisticos verificamos que no ano de 2021 teria havido uma melhoria de cerca de 80\% das vezes, usando qualquer um dos métodos apresentados.\\
% Embora a alocação em falta seja de uma ordem de grandeza superior.\\


% \section{Treino e Resultados\label{se:training}}

% Realizaram-se várias experiências, onde em cada um se ia elimando alguns dos objectos em estudo.\\
% Em casa experiências toda a parametrização era igual, à excepção do objecto de estudo.\\

% \subsection{Arquiteturas e numeros de epocas}

% Nesta experiência foi testado o resultado das várias arquiteturas em estudo, como também o impacto do número de epocas na qualidade dos modelos.\\
% As arquitecturas estudadas foram:

% %TODO: arranjar reseach para cada uma 

% \begin{itemize}
%     \item[--] VanillaDense
%     \item[--] VanillaCNN
%     \item[--] VanillaLSTM
%     \item[--] StackedCNN
%     \item[--] StackedLSTM
%     \item[--] EncoderDecoder
%     \item[--] UNET
% \end{itemize}

% O modelos foram treinados em 200 epocas, sendo que foram salvos a cada 10 epocas, de forma conseguirmos perceber os contextos nos saltos de epocas.\\

% As parametrizações usadas:
% \begin{itemize}
%     \item[--] loss : mean squared error
%     \item[--] Metodo activação no meio : relu
%     \item[--] Metodo activação no fim : relu
%     \item[--] optimizador : Adam
%     \item[--] Janela temporal em X : 168 horas (1 semana)
%     \item[--] Janela temporal em Y : 24 horas (1 semana)
%     \item[--] Fracção de treino : 95\%  
% \end{itemize}

% \subsection{Funções de Perda (Loss)}

% TODO: o que é a loss function?

% Esta experiência consiste em rever que função de perda é melhor aplicavel ao problema. Sendo um problema de regressao linear, de valores bastante oscilatórios e com uma distribuição exponencial, temos algumas loss functions que já são reconhecidas para o problema.\\



% \begin{itemize}
%     \item[--] mean absolute error
%     \item[--] mean squared error    
%     \item[--] loss : mean absolute error

% \end{itemize}



% \subsection{Hiperparametrização}

% \subsubsection{Activação}

% \subsubsection{Optimizadores}

% \subsection{Janelas Temporais}

% Um dos pontos deste trabalho é perceber a fesiabilidade de usar dados de previsão do dia anterior (DA) para estes atributos energéticos.\\
% Algo que pode ser também aplicado no futuro a outros dados que não DA, mas sim a 3 horas, ou a 8 horas.\\
% Para perceber esta flexibilidade, mas especialmente para escolher as melhores janelas temporais a usar neste modelos, vamos testar várias combinações.\\
% Mantendo em mente que o objectivo é prever 24 horas, para os casos onde o alvo não dá um previsão de 24 horas, é necessario criar um número de modelos para fazer as 24 horas.\\
% Para validação apenas é usado o espaço temporar previsto, e não multiplos modelos.\\
% Dado as análises de autocorrelação iremos usar como janelas para treino o conjunto [24, 48, 98, 168] para prever o conjunto [1, 4, 8, 12, 24]\\
% Para alem destes foram também testadas combinações com janelas de treino 8 e 12 horas. Estas mostraram rapidamente que janelas de treino menores que as de previsão funcionam muito mal.\\

% \subsection{Classificação}

% Como descrito em (ref)... existe também o uso de tanto classes como valores linears para resolução de problemas de regressão, também chamado \textit{cluster-wise regression}.\\
% Para este teste mudamos um pouco o modelo em uso. Ao invés de apenas uma camada interpretativa, fazemos duas, em paralelo, sendo que uma resolve a regressão e a outra a classificação.\\
% Outro caso, proposto aqui, é usar uma nova camada interpretativa, que combina as duas saidas anteriores (linear e classificação), e resolve novamente para os valores lineares.\\
% Estes modelos não teram apenas uma saida, mas varias, como as arquiteturas MultiTail, mas neste caso cada uma resolve para um problema diferente, com funções de perda, e activações diferentes.\\

% TODO: desenho destas duas camadas intrepretaticas

% \subsection{Pesos}

% Por ultimo foi testado o impacto do uso de pesos nos modelos. Estes pesos são o peso que aquele alvo TODO: epxlicar pesos

% \subsubsection{Modelos lineares}

% Para os modelos lineares o peso que é adiciona ao modelo é a distância à média.\\
% Este peso serve para dar mais importância a valores facilmente considerados outliers.\\



% \subsubsection{Modelos Lineares e de Classificação}
% Aqui o peso é dado por saida. Para as saidas lineares o peso dados é o mesmo que apresentado anteriormente, para a saida de classificação, o peso é o inverso da frequência da classe.\\
% Distribuindo assim a importância de treino pela frquência das classes. Sendo um prática comum especialmente quando as distribuiçoes são muito desiguais, como o caso em estudo.\\
% É aqui estudada a aplicação destes pesos individualmente, e em conjunto. \\
% Os pesos aqui são também normalizados de modo a que o maior peso em cada um deles seja 1, e logo a multiplicação dos dois esteja dentro das mesmas dimensões de relevância.\\


% \begin{equation} \label{eq:peso_media} 
%     P_m = \left| y - mean \right| 
% \end{equation}

% \section{Considerações adicionais  \label{se:metodos_plus}}

% Foram realizados testes adicionais que não obtiveram resultados passivos de boa interpretação, e foram imediatamente descartados, como:

% \begin{itemize}
%     \item[--] Janela temporal em X : 96, 48, 24
%     \item[--] optimizador : todos os optimizadores disponiveis na biblioteca keras
%     \item[--] loss : todas as outra loss functions de regressão disponiveis.
%     \item[--] epocas : influência do número de epocas nos modelos, foram treinados até 20000 epocas alguns modelos mas à medida que a perda ia estagnando na assintomta, o modelo ia apenas piorando.
% \end{itemize}


% Todos os metodos foram realizados utilizando código em python, que está aberto em https://github.com/JotaFan/renewable-generation-into-reserve-markets
