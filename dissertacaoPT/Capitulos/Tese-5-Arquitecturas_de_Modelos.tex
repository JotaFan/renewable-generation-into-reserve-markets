\chapter{Arquitecturas de Modelos\label{ch:arquiteturas_modelos}}

Grande parte da literatura sobre previsões em modelos de apredizagem apresenta as mesmas arquiteturas, sendo que são depois aprimoradas consoate os dados e o problema. \\
Apresento aqui as aquiteturas mais usadas em previsões, como tambem algumas usadas noutros ramos tentado prever a compatibilidade neste problema. \\
As arquitecturas irão seguir um esquema logíco comum, um bloco de camadas de entrada, um bloco principal e um por fim um bloco interpretativo. \\
As dimensionalidades destas camadas é o que irá formar as diferentes arquitecturas em estudo. \\

\section{Camadas\label{se:layers}}

Para uma construção de modelos usando a ferramenta \href{https://keras.io/}{keras} a unidade básica são as camadas. Estas representação um operação, com uma entrada, e uma saida, e com possiveis parametrizaçoes específicas. \\
Estas camadas ligadas entre si, perfazem um \"profundo\" de camadas neuronais, chamado profundo pois tem mais  que uma camada. \\

Apresento aqui as camadas utilizadas nos modelos aplicados.\\

\subsection{Dense\label{se:dense_layer}}

A camada dense pega num input, cria um numero de neurónios, \textit{N}, também chamado numero de filtros, onde cada neuronio (filtro), recebe informação de cada uma das entradas, e todos os neuronios ligam a todas as dimensões de saida. \\
Cada neuronio gera uma operação, inicialmente aleatoria, para tentar reproduzir uma função que traduza a entrada na saida ideal. \\
Esta camade é altamente influenciada pelo \textit{Perceptão} inicialmente proposto por Franck Rosenblatt\cite{Rosenblatt1958}. Este apresentava um \textit{Perceptão} que fazia uma decisão binária baseado na somas pesadas de todas as entradas. \\
A idea é a base utilizada actualmente, mas apresentava alguma limitações, e muita computação, o proposto por Minsky and Papert\cite{Minsky1969}, eleva a idea com a introdução da funcção de activação e o bias. \\
A utilizção mais recorrente actual é a proposta por Haykin\cite{Haykin1999}, que baseada nas anteriores tem a seguinte apresentação:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Imagens/percepton.png}
	\caption[Ilustração da camada de Dense.]{Adaptado de \cite{Haykin1999}}
	\label{fig:dense}
\end{figure}


\subsection{Convolution\label{se:conv_layer}}

A camada de convoluções difere da dense no sentido em que os filtros (neuronios) não são criados aleatoriamente, mas sim cada filtro trata de uma parte da camada de entrada.
Cada filtro é criado a partir de operações para aquela zona da entrada. Ao conjunto destes filtros dá-se o nome de \textit{feature map}, mapa de atributos, onde normalmente cada filtro aprende a \"ver\" um conceito diferente da entrada. \\
No caso de convoluções em séries temporais, como neste trabalho, os filtros são criados em convoluções temporais\cite{sss}. Pegando no exemplo do trabalho em questão, se usarmos um tamanho da janela de 3 (horas), cada filtro criado terá representaçoes de todos os conjuntos subsequentes de 3 horas. \\
(ref)(TODO: descrever melhor, e provavelmente fazer eu uma imagem para explicar em timeseries).

\begin{figure}[H]
	\centering
	\includegraphics{Imagens/conv_layer.png}
	\caption{Ilustrção da camada de Convulução}
	\label{fig:conv_blcok}
\end{figure}


\subsection{MaxPooling\label{se:max_pooling}}

As camadas de pooling fazem operaçoes para redimensionar os filtros anteriores. \\
Esta camada usada é MaxPooling, que escolhe o maior valor dentro da janela de strides, e aplica na saida. \\
Outros exemplos são Average Pooling ou global Pooling. \\
https://arxiv.org/pdf/2203.01016.pdf

(ref) na imagem
\begin{figure}[H]
	\centering
	\includegraphics{Imagens/pooling.jpg}
	\caption{Ilustrção do efeito da camada de Pooling}
	\label{fig:pooling}
\end{figure}


\subsection{\href{https://keras.io/api/layers/regularization_layers/dropout/}{Dropout}\label{se:dropout}}

Dropout é uma camada que elimina/ignora alguns dos neuronios da camada anterior. Este procedimente impede o overfitting, ajudando na generalização. \\
(ref) na imagem
\begin{figure}[H]
	\centering
	\includegraphics{Imagens/dropout.png}
	\caption{Ilustrção do efeito da camada de dropout}
	\label{fig:dropout}
\end{figure}



\section{Blocos\label{se:blocos}}

Todas as arquiteturas em análise irão ter por base um bloco de camadas neuronais. A formação dessas arquitecturas passa pelas diferentes maneiras que se pode utilizar o bloco principal. Repetições em serie ou em paralelo são um exemplo. \\

\subsection{Bloco Dense\label{se:dense}}

O bloco dense sendo ele o mais simples é formado por duas camadas Dense \ref{se:dense_layer} \cite{}, em que a primeira apresenta um numero maior de filtros que a segunda. \\
Estas camadas não são mais do que uma criação de filtros aleatórios combinando as entradas, para criar todos os filtros de saida. São a base das camadas intrepretativas. A acumulação em série (stacked) de camadas de dense está ligada a melhorias nas capacidades predictivas dos modelos \cite{VLHelen2021}. \\
Exemplo ilustrativo do nosso bloco basico onde entrariam 16 filtros na primeira camada e para finalizar o bloco com 2 filtros \\

\begin{figure}[H]
	\centering
	\includegraphics{Imagens/dense_layer.png}
	\caption{Bloco Dense}
	\label{fig:dense_blcok}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics{Imagens/dense_block.png}
	\caption{Ilustrção do bloco de Dense}
	%\label{fig:dense_blcok}
\end{figure}

\subsection{Bloco CNN\label{se:cnn}}

Bloco de CNN é aqui definido como uma convolução na dimensão temporal seguido de camadas para combater o overfitting, MaxPooling e Dropout. \\
Normalmente usada em processamentos de imagens, o uso de convuluções temporais é tambem por si mesmo uma ideia forte. \\

explicar om que e CNN
imagem

Usada tambem as ideias de attention, residual e o que eu chamei broad

%figura otima: https://www.researchgate.net/publication/344229502_A_Novel_Deep_Learning_Model_for_the_Detection_and_Identification_of_Rolling_Element-Bearing_Faults/figures?lo=1
TODO citar
\begin{figure}[H]
	\centering
	\includegraphics{Imagens/cnn_block.png}
	\caption{Ilustrção do bloco de CNN}
	\label{fig:cnn_block}
\end{figure}


\subsection{Bloco LSTM\label{se:lstm}}

O uso de LSTM para previsões é uma area comum, mas aqui é seguido através das ideas partilhas em \cite{Hewamalage2021}, e reforçado pelo uso em previsões energéticas demonstados em \cite{Costa2022} \\
O bloco LSTM é a aplicaçao das RNN, aqui sendo apenas definido como uma camada de LSTM. \\
Estes blocos mantêm dentro de si ligações a diferentes camadas temporais, e cada filtro criado, mantêm uma "memória" dos filtros passados. \\
Bastante utilizado em modelação de linguagem.

imagem


\section{Arquiteturas \label{se:arquitecturas}}

\subsection{Vanilla \label{se:vannila}}

O termo "Vanilla" aqui é aplicado para aquitecturas que apenas usam um bloco de cada, um de entrada, um principal, e um interpretativo. \\
Como exemplo a arquitetura de "VanillaCNN"

imagem da mesma

\subsection{Stacked\label{se:stacked}}

Stacked refere-se a "amontoado" onde se utiliza o bloco principal várias vezes em série.E apenas um bloco de  entrada e um interpretativo. \\
Como exemplo a arquitetura de "StackedCNN"

imagem da mesma

\subsection{MultiHead\label{se:multihea}}

Multihead é o termo para quando os blocos de entrada e principais são repetidos paralelamente, um caminho para cada atributo, ou uma outra paralelização à escolha. Sendo depois concatenadas essas camadas e passadas juntas para a camada interpretativa. \\
Aqui foi usado sempre a paralelização por atributos, e ao invês de fazer Mulithead no sentido de multiplas entradas, para simplicidade de programação, foi feito um paralização interna no modelo, apos a camada de entrada, onde a mesma é repetida para cada atributo. \\
Foi testado a diferença, e para os dados usados não havia diferenças de qualidade, mas sim em tempo de treino, logo a mais rapida foi a escolhida. \\
Como exemplo a arquitetura de "MultiheadCNN"

imagem da mesma

\subsection{MultiTail\label{se:multitail}}

Esta arquitectura tem o mesmo conceito que a anterior a nivel de paralelização, mas neste caso esta é feita apenas na camada interpretativa. Sendo que o resultado do bloco principal é repetido para criar a paralelização. \\
Neste caso foi paralelizado com o numero de tempos a prever, 24 horas, 24 objectos de saida destas modelos.  \\
A grande diferença desta arquitectura para a "Vanilla" que preve 24 horas, é que aqui cada hora tem o seu proprio valor de função de perda, logo o modelo como que está a treinar 24 modelos diferentes, e no caso "Vanilla" a função de perda é ùnica e é a media do erro das horas todas. \\
Como exemplo a arquitetura de "MultiTailCNN"

imagem da mesma

\subsection{UNET\label{se:UNET}}

Normalmente usando em modelção de imagens, a arquitectura UNET passa por criar uma rede de expansão dos filtros, usando convoluções, e de seguida uma rede de contracção dos mesmo, até aos tamanhos pretendidos.\\
O bloco principal contextualmente o mesmo que o CNN.\\
Nas suas ligações UNET junta informação de filtros passados (não de nivel temporal mas de rede neuronal) para realçar informação já trabalhada, e assim identificar padrões de vários contextos diferentes.\\
É habitual tambem adicionar aos blocos principais portões de atenção, portões residuais. Estas duas tecnicas são tambem estudadas aqui.\\
É chamada assim pois é uma rede (NET) que forma um U na sua expansão e contracção.\\

Como exemplo a arquitetura de "UNET"

imagem da mesma


\section{Considerações adicionais\label{se:modelos_plus}}

Aqui e dizer que os modelos utilizados para teste sao as combinacoes deste blocos nestas aquiteturas.

Imagens de layers criadas com 
dense
http://alexlenail.me/NN-SVG/index.html